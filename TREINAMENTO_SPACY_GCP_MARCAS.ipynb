{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TREINAMENTO_SPACY/GCP_MARCAS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johndelara1/AIRBNB-cognitivo/blob/master/TREINAMENTO_SPACY_GCP_MARCAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QMFiB0KZkMk"
      },
      "source": [
        "# Proposta do SPACY?\n",
        "- Com o RE conseguimos identificar bem as palavras que temos mapeadas, mas não conseguimos mapear duas situações: marcas que não estão na nossa lista de \"MarcaPalavras\" (novas marcas) e nem casos onde não temos sinonimos da marca na descrição\n",
        "\n",
        "- Proposta: utilizar o Spacy para identificarmos essas duas situações que não são mapeadas pelo RE (novas marcas) e marcas sem a palavra \"Marca\" antes do nome da marca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8RynPyjPA5F",
        "outputId": "0079f060-e483-404e-f1f3-816e20a15d39"
      },
      "source": [
        "#!pip install spacy-pytorch-transformers[cuda100]==0.2.0\n",
        "# !pip install spacy_sentiws\n",
        "#!pip install spacy==2.3.2\n",
        "#!python -m spacy download pt_core_news_lg\n",
        "#!python -m spacy link pt_core_news_lg pt_core_news_lg\n",
        "# !pip install -U spacy[cuda100]\n",
        "# pip install cupy-cuda100\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive')\n",
        "import pandas as pd\n",
        "import re\n",
        "import funcoes\n",
        "import spacy\n",
        "import glob\n",
        "spacy.require_gpu()\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "import random\n",
        "# !pip install google-cloud-automl\n",
        "# from google.cloud import automl\n",
        "# nlp = spacy.load(\"/content/gdrive/MyDrive/train_10000_rows/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "bibliotecas carregadas!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWHZ3UiXDdfi"
      },
      "source": [
        "# Testar o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7a4itjw8ZIIr",
        "outputId": "78d1b298-6000-4c0d-a51f-4e446eed5e81"
      },
      "source": [
        "'ec b aparelho conversor protocolos tipo gateway interconexao redes comunicacao marca lonwork bacnet utilizadas respectivamente sistemas alarme protecao contra incendios sistemas gestao predial bms referenc'\n",
        "# OK para palavras depois de marca\n",
        "'pcs software maintenance station gx yb oq cdp icss software upgrade engineering v floating license f siemens user swcomponent djf system supervisory paraty serial,1'\n",
        "# ERRO pois achou 'maintenance'\n",
        "# # ERRO pois achou 'cdp'\n",
        "'es tg ab siemens s unidade processamento central plc tecnologia memoria trabalho kbyte integrada conector dianteiro tecnologico x pin cartao memoria micro min mb necessarios,1'\n",
        "# OK siemens\n",
        "'fe bb roteador digital nokia marca isam capacidade conexao fio aplicado redes opticas marca nokia ref g w h plug brazil'\n",
        "# trouxe previsões mais voltados a marca 'nokia' verificamos mais exemplos no treinamento\n",
        "# trouxe a marca isam no segundo modelo\n",
        "'marca  consul câmera segurança marca intelbras ic ns s ug j'\n",
        "# Boa no novo modelo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'marca  consul câmera segurança marca intelbras ic ns s ug j'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHuiF1_1eQLa",
        "outputId": "a3e24312-d7a7-46be-f923-d8247b229c40"
      },
      "source": [
        "test_text = 'fe bb roteador digital nokia marca isam capacidade conexao fio aplicado redes opticas marca nokia ref g w h plug brazil'\n",
        "doc = nlp(test_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "isam 35 39 category\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAgI9mBXDZTu"
      },
      "source": [
        "#  Treinar Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMhtvHvNRisC"
      },
      "source": [
        "def uploadCSV(path):\n",
        "    df = pd.read_csv(path, encoding='latin_1', sep=';')\n",
        "    return df\n",
        "def train_spacy(data, iterations):\n",
        "    TRAIN_DATA = data\n",
        "    nlp = spacy.blank('pt')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "    \n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training(device=-1)#device=-1 ou vazio -> https://support.prodi.gy/t/will-a-gpu-make-training-faster/187/3\n",
        "        for itn in range(iterations):\n",
        "            print(\"Statring iteration \" + str(itn))\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.2,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "    return nlp\n",
        "\n",
        "def update_train_spacy(data, iterations):\n",
        "    TRAIN_DATA = data\n",
        "    nlp = spacy.load(\"/content/gdrive/MyDrive/train_10000_rows/\")\n",
        "    \n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.resume_training(device=-1)\n",
        "        for itn in range(iterations):\n",
        "            print(\"Statring iteration \" + str(itn))\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.2,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "    return nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WUjq23JSP7-"
      },
      "source": [
        "# TRAIN_DATA = list()\n",
        "# for arquivo in range(0, len(files)):\n",
        "#     df = uploadCSV(files[arquivo])\n",
        "#     # df = df.drop_duplicates(subset=['text']).reset_index()\n",
        "#     # df.text = df.test.map(lambda x: x.lower())\n",
        "#     for i in range(0,len(df)):\n",
        "#         if pd.notna(df['start'][i]) and pd.notna(df['finish'][i]):\n",
        "#             # print(i, '|' * 50)\n",
        "#             # print(df['Descricao'][i], \"\\n\", df['Marca'][i], '=>', df['Marca Posição inicial'][i],  '|', df['Marca Posição Final'][i], '\\n', df['Modelo'][i], '=>', df['Modelo Posição inicial'][i],  '|', df['Modelo Posição Final'][i])\n",
        "#             # texto = df['Descricao'][i]\n",
        "#             # print(texto[int(df['Marca Posição inicial'][i]):int(df['Marca Posição Final'][i])])\n",
        "#             # print(texto[int(df['Modelo Posição inicial'][i]):int(df['Modelo Posição Final'][i])])\n",
        "#             # print('-'* 50)\n",
        "#             # print(df['Descricao'][i], int(df['Marca Posição inicial'][i]), int(df['Marca Posição Final'][i]))\n",
        "#             TRAIN_DATA.append((df['text'][i], {'entities': [(int(df['Marca Posição inicial'][i]), int(df['Marca Posição Final'][i]), 'MARCA')]}))\n",
        "#             # TRAIN_DATA = list(set(TRAIN_DATA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiNEw4tGR6bK",
        "outputId": "680dc645-231d-44e7-b799-5103d2e362f8"
      },
      "source": [
        "# files = glob.glob(\"/content/drive/MyDrive/Bases Para Dicionário/*.csv\")\r\n",
        "%%time\r\n",
        "df = funcoes.simple.select_dw(\"SELECT * FROM `simplediscoverydata.consumerZone.DS_GERAL_S_70000-MarcasParaTreinamentoSpacy` WHERE id_treinoETeste <= 20000\")\r\n",
        "TRAIN_SPACY = list()\r\n",
        "for i in range(0,len(df)):\r\n",
        "  if pd.notna(df['start'][i]) and pd.notna(df['finish'][i]):\r\n",
        "    TRAIN_SPACY.append((df['text'][i], {'entities': [(int(df['start'][i]), int(df['finish'][i]), 'category')]}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.12 s, sys: 35.2 ms, total: 1.15 s\n",
            "Wall time: 4.25 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MywWEO_kXeJK",
        "outputId": "6b2c110e-05b2-4a50-fc59-2a53da19726e"
      },
      "source": [
        "%%time\n",
        "# Treinamento marca\n",
        "prdnlp = update_train_spacy(TRAIN_SPACY, 20)\n",
        "\n",
        "# Save our trained Model\n",
        "modelfile = '/content/gdrive/MyDrive/train_20000_rows_/'\n",
        "prdnlp.to_disk(modelfile)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statring iteration 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOR_paim5sv1"
      },
      "source": [
        "# nlp = spacy.load(\"/content/gdrive/MyDrive/marcas\")\n",
        "# nlp = spacy.load(\"marcas_intelbras\")\n",
        "# nlp = spacy.load(\"train_1000_rows\")\n",
        "# nlp = spacy.load(\"/content/gdrive/MyDrive/train_10000_rows/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSXa-NOoDEgl"
      },
      "source": [
        "# Tratativas de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd74iP6-QElQ"
      },
      "source": [
        "def remove_stopwords_fast(text):\n",
        "    doc = nlp(text.lower())\n",
        "    result = [token.text for token in doc if token.text not in nlp.Defaults.stop_words]\n",
        "    return \" \".join(result)\n",
        "def remove_stopwords(text):\n",
        "    doc = nlp(text.lower()) #1\n",
        "    result = [] #2\n",
        "    for token in doc: #3\n",
        "        if token.text in nlp.Defaults.stop_words: #4\n",
        "            continue\n",
        "        result.append(token.text)#5\n",
        "    return \" \".join(result) #6\n",
        "def remove_pronoun(text):\n",
        "    doc = nlp(text.lower())\n",
        "    result = [token for token in doc if token.lemma_ != '-PRON-']\n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "def process_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    result = []\n",
        "    for token in doc:\n",
        "        if token.text in nlp.Defaults.stop_words:\n",
        "            continue\n",
        "        if token.is_punct:\n",
        "            continue\n",
        "        if token.lemma_ == '-PRON-':\n",
        "            continue\n",
        "        result.append(token.lemma_)\n",
        "    return \" \".join(result)\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    base = nlp(process_text(text1))\n",
        "    compare = nlp(process_text(text2))\n",
        "    return base.similarity(compare)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yddKlwvqCkV2"
      },
      "source": [
        "# GCP - Para treinarno GCP extração de variáveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifd-O5KZdlVY"
      },
      "source": [
        "def create_dataset(project_id, display_name):\r\n",
        "    \"\"\"Create a dataset.\"\"\"\r\n",
        "    # [START automl_language_entity_extraction_create_dataset]\r\n",
        "    from google.cloud import automl\r\n",
        "\r\n",
        "    # TODO(developer): Uncomment and set the following variables\r\n",
        "    # project_id = \"YOUR_PROJECT_ID\"\r\n",
        "    # display_name = \"YOUR_DATASET_NAME\"\r\n",
        "\r\n",
        "    client = automl.AutoMlClient()\r\n",
        "\r\n",
        "    # A resource that represents Google Cloud Platform location.\r\n",
        "    project_location = f\"projects/{project_id}/locations/us-central1\"\r\n",
        "    metadata = automl.TextExtractionDatasetMetadata()\r\n",
        "    dataset = automl.Dataset(\r\n",
        "        display_name=display_name, text_extraction_dataset_metadata=metadata\r\n",
        "    )\r\n",
        "\r\n",
        "    # Create a dataset with the dataset metadata in the region.\r\n",
        "    response = client.create_dataset(parent=project_location, dataset=dataset)\r\n",
        "\r\n",
        "    created_dataset = response.result()\r\n",
        "\r\n",
        "    # Display the dataset information\r\n",
        "    print(\"Dataset name: {}\".format(created_dataset.name))\r\n",
        "    print(\"Dataset id: {}\".format(created_dataset.name.split(\"/\")[-1]))\r\n",
        "    # [END automl_language_entity_extraction_create_dataset]\r\n",
        "\r\n",
        "def create_model(project_id, dataset_id, display_name):\r\n",
        "    \"\"\"Create a model.\"\"\"\r\n",
        "    # [START automl_language_entity_extraction_create_model]\r\n",
        "    from google.cloud import automl\r\n",
        "\r\n",
        "    # TODO(developer): Uncomment and set the following variables\r\n",
        "    # project_id = \"YOUR_PROJECT_ID\"\r\n",
        "    # dataset_id = \"YOUR_DATASET_ID\"\r\n",
        "    # display_name = \"YOUR_MODEL_NAME\"\r\n",
        "\r\n",
        "    client = automl.AutoMlClient()\r\n",
        "\r\n",
        "    # A resource that represents Google Cloud Platform location.\r\n",
        "    project_location = f\"projects/{project_id}/locations/us-central1\"\r\n",
        "    # Leave model unset to use the default base model provided by Google\r\n",
        "    metadata = automl.TextExtractionModelMetadata()\r\n",
        "    model = automl.Model(\r\n",
        "        display_name=display_name,\r\n",
        "        dataset_id=dataset_id,\r\n",
        "        text_extraction_model_metadata=metadata,\r\n",
        "    )\r\n",
        "\r\n",
        "    # Create a model with the model metadata in the region.\r\n",
        "    response = client.create_model(parent=project_location, model=model)\r\n",
        "\r\n",
        "    print(\"Training operation name: {}\".format(response.operation.name))\r\n",
        "    print(\"Training started...\")\r\n",
        "    # [END automl_language_entity_extraction_create_model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjbwZDkO0SHq"
      },
      "source": [
        "# import os\r\n",
        "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/api_google.json\"\r\n",
        "# create_dataset(project_id='simplediscoverydata', display_name='simpleDicoveryData')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjN6tsPP--Y_"
      },
      "source": [
        "# create_model(project_id='simplediscoverydata', dataset_id='TEN3449098157344948224', display_name='simpleDicoveryData')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNQLwLVTe9D3"
      },
      "source": [
        "# jsonl = df.to_json(orient='records', lines=True, force_ascii=False)\r\n",
        "# # !pip install jsonlines\r\n",
        "# import jsonlines\r\n",
        "# g = open('data.jsonl', 'w')\r\n",
        "# with g as outfile:\r\n",
        "#   json.dump(jsonl, outfile, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}